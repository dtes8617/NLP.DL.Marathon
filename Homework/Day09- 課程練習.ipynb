{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128, 64)\n",
    "        self.layer3 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=500,output_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.linear.0.weight torch.Size([128, 500])\n",
      "layer1.linear.0.bias torch.Size([128])\n",
      "layer1.linear.2.weight torch.Size([128])\n",
      "layer1.linear.2.bias torch.Size([128])\n",
      "layer2.linear.0.weight torch.Size([64, 128])\n",
      "layer2.linear.0.bias torch.Size([64])\n",
      "layer2.linear.2.weight torch.Size([64])\n",
      "layer2.linear.2.bias torch.Size([64])\n",
      "layer3.linear.0.weight torch.Size([32, 64])\n",
      "layer3.linear.0.bias torch.Size([32])\n",
      "layer3.linear.2.weight torch.Size([32])\n",
      "layer3.linear.2.bias torch.Size([32])\n",
      "output.linear.0.weight torch.Size([1, 32])\n",
      "output.linear.0.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name,_ in model.named_parameters():\n",
    "    print(name, _.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "input_features = 500\n",
    "dummy_input = torch.randn(batch_size, input_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3981],\n",
      "        [0.4823],\n",
      "        [0.6930],\n",
      "        [0.5917],\n",
      "        [0.4467],\n",
      "        [0.5056],\n",
      "        [0.5486],\n",
      "        [0.5445],\n",
      "        [0.5141],\n",
      "        [0.7055],\n",
      "        [0.6723],\n",
      "        [0.4662]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1981, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.7327, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCELoss, BCEWithLogitsLoss\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "prediction = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "\n",
    "## 將每一組 prediciotn, target 算出來的值相加\n",
    "criterios = BCELoss(reduction='sum')\n",
    "loss_1 = criterios(sigmoid(prediction), target)\n",
    "\n",
    "## 將每一組 prediciotn, target 算出來的值平均\n",
    "criterios = BCELoss(reduction='mean')\n",
    "loss_2 = criterios(sigmoid(prediction), target)\n",
    "\n",
    "print(loss_1, loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7327, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "## BCEWithLogitsLoss自帶 sigmoid 功能\n",
    "criterios = BCEWithLogitsLoss(reduction='mean')\n",
    "loss_3 = criterios(prediction, target)\n",
    "assert loss_2 == loss_3 ## 應該要與 output2相同\n",
    "print(loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CrossEntropyLoss = LogSoftmax + NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.randn(2, 3)\n",
    "ground_truth = torch.tensor([2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-38f2dc36d86f>:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = log_softmax(prediction)\n",
      "<ipython-input-47-38f2dc36d86f>:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  assert '{:.4f}'.format(output.sum()) == '{:.4f}'.format(torch.log(softmax(prediction)).sum())\n"
     ]
    }
   ],
   "source": [
    "log_softmax = LogSoftmax()\n",
    "\n",
    "output = log_softmax(prediction)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "assert '{:.4f}'.format(output.sum()) == '{:.4f}'.format(torch.log(softmax(prediction)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9151)\n"
     ]
    }
   ],
   "source": [
    "criterion = NLLLoss()\n",
    "loss_1 = criterion(output, ground_truth)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 將NLLLoss拆解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9151)\n"
     ]
    }
   ],
   "source": [
    "ground_truth_onehot = torch.FloatTensor(prediction.shape)\n",
    "ground_truth_onehot.zero_()\n",
    "ground_truth_onehot.scatter_(1, ground_truth.reshape(-1,1), 1)\n",
    "loss_count = - torch.mul(ground_truth_onehot, output).sum(-1).mean()\n",
    "assert '{:.4f}'.format(loss_1) == '{:.4f}'.format(loss_count)\n",
    "print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9151)\n"
     ]
    }
   ],
   "source": [
    "loss_2 = criterion(prediction, ground_truth)\n",
    "assert '{:.4f}'.format(loss_1) == '{:.4f}'.format(loss_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE == L2 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['pic'](l1_l2_smooth.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss, L1Loss, SmoothL1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2605, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = MSELoss()\n",
    "output = criterion(prediction, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8954, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = L1Loss()\n",
    "output = criterion(prediction, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SmoothL1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5182, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = SmoothL1Loss()\n",
    "output = criterion(prediction, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=500,output_classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* params : iterable of parameters\n",
    "* lr : learning rate\n",
    "* weight_decay : (L2) Regularization (正則化) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(params=model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-3)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "optimizer = optim.RMSprop(params=model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0,\\\n",
    "                          momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用 optimizer.step() 來實現參數更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "input_features = 500\n",
    "dummy_input = torch.randn(batch_size, input_features)\n",
    "\n",
    "prediction = model(dummy_input)\n",
    "target = torch.empty(12, dtype=torch.float).random_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0143, -0.0278,  0.0318,  ...,  0.0302, -0.0134,  0.0200],\n",
      "        [ 0.0241,  0.0418, -0.0318,  ...,  0.0057,  0.0250,  0.0087],\n",
      "        [-0.0024,  0.0388, -0.0266,  ...,  0.0007,  0.0086, -0.0360],\n",
      "        ...,\n",
      "        [-0.0131,  0.0092,  0.0143,  ..., -0.0247,  0.0072, -0.0266],\n",
      "        [-0.0303,  0.0079,  0.0153,  ...,  0.0137, -0.0331, -0.0120],\n",
      "        [ 0.0370, -0.0115, -0.0273,  ..., -0.0003,  0.0298, -0.0254]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : None\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterios = BCELoss(reduction='mean')\n",
    "loss = criterios(prediction.reshape(-1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0143, -0.0278,  0.0318,  ...,  0.0302, -0.0134,  0.0200],\n",
      "        [ 0.0241,  0.0418, -0.0318,  ...,  0.0057,  0.0250,  0.0087],\n",
      "        [-0.0024,  0.0388, -0.0266,  ...,  0.0007,  0.0086, -0.0360],\n",
      "        ...,\n",
      "        [-0.0131,  0.0092,  0.0143,  ..., -0.0247,  0.0072, -0.0266],\n",
      "        [-0.0303,  0.0079,  0.0153,  ...,  0.0137, -0.0331, -0.0120],\n",
      "        [ 0.0370, -0.0115, -0.0273,  ..., -0.0003,  0.0298, -0.0254]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0190, -0.0331,  0.0194,  ..., -0.0426, -0.0159,  0.0356],\n",
      "        [ 0.0039, -0.0019,  0.0019,  ...,  0.0019,  0.0008,  0.0002],\n",
      "        [ 0.0067,  0.0125, -0.0092,  ...,  0.0172, -0.0085,  0.0047],\n",
      "        ...,\n",
      "        [-0.0137, -0.0029, -0.0002,  ..., -0.0037,  0.0046,  0.0098],\n",
      "        [-0.0012, -0.0015, -0.0001,  ..., -0.0021, -0.0006,  0.0027],\n",
      "        [-0.0035,  0.0116, -0.0013,  ..., -0.0050,  0.0114,  0.0221]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.0857,  0.0722, -0.0682,  ...,  0.1302,  0.0866, -0.0800],\n",
      "        [-0.0759,  0.1418, -0.1318,  ..., -0.0943, -0.0750, -0.0912],\n",
      "        [-0.1024, -0.0612,  0.0734,  ..., -0.0993,  0.1086, -0.1360],\n",
      "        ...,\n",
      "        [ 0.0869,  0.1092,  0.1143,  ...,  0.0753, -0.0928, -0.1266],\n",
      "        [ 0.0696,  0.1079,  0.1152,  ...,  0.1137,  0.0668, -0.1120],\n",
      "        [ 0.1370, -0.1115,  0.0727,  ...,  0.0997, -0.0702, -0.1254]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0190, -0.0331,  0.0194,  ..., -0.0426, -0.0159,  0.0356],\n",
      "        [ 0.0039, -0.0019,  0.0019,  ...,  0.0019,  0.0008,  0.0002],\n",
      "        [ 0.0067,  0.0125, -0.0092,  ...,  0.0172, -0.0085,  0.0047],\n",
      "        ...,\n",
      "        [-0.0137, -0.0029, -0.0002,  ..., -0.0037,  0.0046,  0.0098],\n",
      "        [-0.0012, -0.0015, -0.0001,  ..., -0.0021, -0.0006,  0.0027],\n",
      "        [-0.0035,  0.0116, -0.0013,  ..., -0.0050,  0.0114,  0.0221]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.0857,  0.0722, -0.0682,  ...,  0.1302,  0.0866, -0.0800],\n",
      "        [-0.0759,  0.1418, -0.1318,  ..., -0.0943, -0.0750, -0.0912],\n",
      "        [-0.1024, -0.0612,  0.0734,  ..., -0.0993,  0.1086, -0.1360],\n",
      "        ...,\n",
      "        [ 0.0869,  0.1092,  0.1143,  ...,  0.0753, -0.0928, -0.1266],\n",
      "        [ 0.0696,  0.1079,  0.1152,  ...,  0.1137,  0.0668, -0.1120],\n",
      "        [ 0.1370, -0.1115,  0.0727,  ...,  0.0997, -0.0702, -0.1254]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_marathon",
   "language": "python",
   "name": "nlp_marathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
